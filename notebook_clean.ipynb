{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wADwK78DCz"
      },
      "source": [
        "# Proyek Klasifikasi Gambar: Sea Animals Dataset\n",
        "- **Nama:** Isnaini Cahyaningrum\n",
        "- **Email:** isnaini.anicn@gmail.com\n",
        "- **ID Dicoding:** anicn14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-z4QGlO8DC1"
      },
      "source": [
        "## Import Semua Packages/Library yang Digunakan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVYwaObI8DC1"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm as tq\n",
        "\n",
        "# Libraries untuk pemrosesan data gambar\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.util import random_noise\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Libraries untuk pembangunan model\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m ipykernel install --user --name=main-ds --display-name \"main-ds (Python 3.9.21)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK4DvqfbYrN8"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHekw29KX4XQ"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Membuat kamus untuk menyimpan gambar setiap kelas dalam data\n",
        "animals_image = {}\n",
        "\n",
        "# Tentukan path sumber train\n",
        "path = \"../sea-animals-dataset/dataset/\"\n",
        "# path_sub = os.path.join(path \"dataset\")\n",
        "for i in os.listdir(path):\n",
        "    animals_image[i] = os.listdir(os.path.join(path, i))\n",
        "\n",
        "# Menampilkan secara acak 5 gambar setiap dari 16 kelas data\n",
        "fig, axs = plt.subplots(len(animals_image.keys()), 5, figsize=(30, 30))\n",
        "\n",
        "for i, class_name in enumerate(os.listdir(path)):\n",
        "    images = np.random.choice(animals_image[class_name], 5, replace=False)\n",
        "    \n",
        "    for j, image_name in enumerate(images):\n",
        "        img_path = os.path.join(path, class_name, image_name)\n",
        "        img = Image.open(img_path).convert(\"L\") # Konversi menjadi skala keabuan\n",
        "        axs[i, j].imshow(img, cmap='gray')\n",
        "        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot Distribution "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisikan path sumber\n",
        "animals_path = \"../sea-animals-dataset/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(animals_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFgLyQPHX98s"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Membuat fungsi rotasi gambar yang dipakai pada fungsi rotasi lainnya\n",
        "def rotate_image(img, angle):\n",
        "    h, w = img.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
        "    return rotated\n",
        "\n",
        "# Membuat fungsi untuk melakukan rotasi berlawanan arah jarum jam\n",
        "def anticlockwise_rotation(img):\n",
        "    return rotate_image(img, random.randint(10, 90))\n",
        "\n",
        "# Membuat fungsi untuk melakukan rotasi searah jarum jam\n",
        "def clockwise_rotation(img):\n",
        "    return rotate_image(img, -random.randint(10, 90))\n",
        "\n",
        "# Membuat fungsi untuk membalik gambar secara vertikal dari atas ke bawah\n",
        "def flip_up_down(img):\n",
        "    return cv2.flip(img, 0)\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek peningkatan kecerahan pada gambar\n",
        "def add_brightness(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    hsv[...,2] = cv2.add(hsv[...,2], 40)\n",
        "    bright = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    return bright\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek blur pada gambar\n",
        "def blur_image(img):\n",
        "    blur = cv2.GaussianBlur(img, (3, 3), sigmaX=0.3)\n",
        "    return blur\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek pergeseran acak pada gambar\n",
        "def sheared(img):\n",
        "    h, w = img.shape[:2]\n",
        "    shear_factor = random.uniform(-0.1, 0.1)\n",
        "    matrix = np.array([[1, shear_factor, 0],\n",
        "                        [0, 1, 0]], dtype=np.float32)\n",
        "    return cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "# Membuat fungsi untuk melakukan pergeseran melengkung pada gambar\n",
        "def warp_shift(img):\n",
        "    h, w = img.shape[:2]\n",
        "    shift_y = random.randint(-20, 20)\n",
        "    matrix = np.array([[1, 0, 0],\n",
        "                        [0, 1, shift_y]], dtype=np.float32)\n",
        "    return cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "# Membuat fungsi untuk melakukan perubahan ukuran gambar\n",
        "def resize_image(img, size=(224, 224)):\n",
        "    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisikan fungsi-fungsi transformasi (pastikan sudah didefinisikan sebelumnya)\n",
        "transformations = {\n",
        "    'rotate anticlockwise': anticlockwise_rotation,\n",
        "    'rotate clockwise': clockwise_rotation,\n",
        "    'warp shift': warp_shift,\n",
        "    'blurring image': blur_image,\n",
        "    'add brightness': add_brightness,\n",
        "    'flip up down': flip_up_down,\n",
        "    'shear image': sheared\n",
        "}\n",
        "\n",
        "base_path = \"../sea-animals-dataset/dataset\"\n",
        "target_per_class = 1700\n",
        "\n",
        "class_image_paths = defaultdict(list)\n",
        "\n",
        "# Buat daftar semua gambar dan simpan nama kelasnya\n",
        "for class_name in os.listdir(base_path):\n",
        "    class_dir = os.path.join(base_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for im in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, im)\n",
        "            class_image_paths[class_name].append(image_path)\n",
        "\n",
        "# Augmentasi hanya untuk kelas dengan < 1700 gambar\n",
        "for class_name, image_paths in class_image_paths.items():\n",
        "    current_count = len(image_paths)\n",
        "    print(f\"Kelas '{class_name}': {current_count} gambar\")\n",
        "\n",
        "    if current_count >= target_per_class:\n",
        "        print(f\"> Lewati, sudah >= {target_per_class}\\n\")\n",
        "        continue\n",
        "\n",
        "    print(f\"> Augmentasi diperlukan: {target_per_class - current_count} gambar\\n\")\n",
        "    i = 1\n",
        "    while current_count < target_per_class:\n",
        "        image_path = random.choice(image_paths)\n",
        "\n",
        "        try:\n",
        "            image = io.imread(image_path)\n",
        "\n",
        "            if image.ndim == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = image[:, :, :3]\n",
        "\n",
        "            if image.dtype != np.uint8:\n",
        "                image = img_as_ubyte(image)\n",
        "\n",
        "            transformed = image.copy()\n",
        "            num_transforms = random.randint(1, len(transformations))\n",
        "            selected_transforms = random.sample(list(transformations), num_transforms)\n",
        "\n",
        "            for key in selected_transforms:\n",
        "                transformed = transformations[key](transformed)\n",
        "\n",
        "            transformed = resize_image(transformed)\n",
        "\n",
        "            if transformed.dtype != np.uint8:\n",
        "                transformed = img_as_ubyte(transformed)\n",
        "            transformed = cv2.cvtColor(transformed, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            save_path = os.path.join(base_path, class_name, f\"{class_name}_aug_{i}.jpg\")\n",
        "            cv2.imwrite(save_path, transformed)\n",
        "            i += 1\n",
        "            current_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Gagal proses {image_path}: {e}, lanjut.\")\n",
        "\n",
        "print(\"\\n✅ Augmentasi selesai. Semua kelas minimal memiliki 1700 gambar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot Distribution After Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisikan path sumber\n",
        "animals_path = \"../sea-animals-dataset/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(animals_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ICO2-E0YxzD"
      },
      "source": [
        "#### Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HJokQbxX98s"
      },
      "outputs": [],
      "source": [
        "# Panggil variabel mypath yang menampung folder dataset hasil augmentasi\n",
        "mypath = \"../sea-animals-dataset/dataset\"\n",
        "\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "for path, subdirs, files in os.walk(mypath):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "# Memasukkan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapih\n",
        "df = pd.DataFrame({\"path\":full_path,\"file_name\":file_name,\"labels\":labels})\n",
        "# Melihat jumlah data pada masing-masing label\n",
        "df.groupby(['labels']).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variabel yang digunakan pada pemisahan data ini adalah x = data path dan y = data labels\n",
        "X= df['path']\n",
        "y= df['labels']\n",
        "\n",
        "# Split dataset menjadi data train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=300\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menyatukan ke masing-masing dataframe\n",
        "df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})\n",
        "df_te = pd.DataFrame({'path':X_test, 'labels':y_test,'set':'test'}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menggabungkan DataFrame df_tr dan df_te\n",
        "df_all = pd.concat([df_tr, df_te], ignore_index=True)\n",
        "\n",
        "print('===================================================== \\n')\n",
        "print(df_all.groupby(['set', 'labels']).size(), '\\n')\n",
        "print('===================================================== \\n')\n",
        "\n",
        "# Cek data sampel\n",
        "print(df_all.sample(5))\n",
        "\n",
        "# Memanggil dataset yang berisi keseluruhan gambar yang sesuai dengan labelnya\n",
        "datasource_path = \"../sea-animals-dataset/dataset_augmented\"\n",
        "# Membuat variabel Dataset, tempat menampung data yang telah dilakukan pembagian data training dan testing\n",
        "dataset_path = \"../Dataset_Final/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_all.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, row in tq(df_all.iterrows()):\n",
        "    # Ambil nama file\n",
        "    image_name = os.path.basename(row['path'])\n",
        "    class_name = os.path.basename(row['labels'])\n",
        "    \n",
        "    # Deteksi filepath\n",
        "    file_path = row['path']\n",
        "    if os.path.exists(file_path) == False:\n",
        "            file_path = os.path.join(datasource_path, row['image'])\n",
        "\n",
        "    # Buat direktori tujuan folder\n",
        "    target_dir = os.path.join(dataset_path, row['set'], class_name)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Tentukan tujuan file\n",
        "    file_dest = os.path.join(target_dir, image_name)\n",
        "\n",
        "    # Salin file dari sumber ke tujuan\n",
        "    if not os.path.exists(file_dest):\n",
        "        shutil.copy2(file_path, file_dest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Image Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisikan direktori training dan test\n",
        "TRAIN_DIR = \"../Dataset_Final/train/\"\n",
        "TEST_DIR = \"../Dataset_Final/test/\"\n",
        "\n",
        "# Buat objek ImageDataGenerator yang menormalkan gambar\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1/255., \n",
        "                                validation_split=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(150,150),\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='training',\n",
        "                                                shuffle=True)\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                    batch_size=32,\n",
        "                                                    target_size=(150,150),\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='validation',\n",
        "                                                    shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                    batch_size=1,\n",
        "                                                    target_size=(150,150),\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(os.listdir(\"../Dataset_Final/\"))\n",
        "print(os.listdir(\"../Dataset_Final/test/\"))\n",
        "print(os.listdir(\"../Dataset_Final/train/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-Ph-oIYAUU"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTwK0t8XYAUU"
      },
      "outputs": [],
      "source": [
        "####################### Init Transfer Learning Model with MobileNetV2 ##################################\n",
        "# Load pretrained MobileNetV2 tanpa top layer (fully connected-nya)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = True  # Freeze semua layer awal dulu\n",
        "\n",
        "# Freeze layer bawah (opsional, misal hanya fine-tune bagian atas)\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "####################### Init sequential model ##################################\n",
        "model_1 = Sequential()\n",
        "\n",
        "# # ######################### Input layer with Fully Connected Layer ################################\n",
        "# # 1st Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPooling2D((2, 2)))\n",
        "# model_1.add(Dropout(0.2))\n",
        "\n",
        "# # 2nd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPooling2D((2, 2)))\n",
        "# model_1.add(Dropout(0.2))\n",
        "\n",
        "# # 3rd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPooling2D((2, 2)))\n",
        "# model_1.add(Dropout(0.3))\n",
        "\n",
        "# # 4rd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "# model_1.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "# model_1.add(BatchNormalization())\n",
        "# model_1.add(MaxPooling2D((2, 2)))\n",
        "# model_1.add(Dropout(0.3))\n",
        "\n",
        "# Base Layer\n",
        "model_1.add(base_model)\n",
        "\n",
        "# Global Average Pooling\n",
        "model_1.add(GlobalAveragePooling2D())\n",
        "\n",
        "# 1nd Dense Layer\n",
        "model_1.add(Dense(256, activation = 'relu'))\n",
        "# 1nd Dropout Layer\n",
        "model_1.add(Dropout(0.5))\n",
        "# 2nd Dense Layer\n",
        "model_1.add(Dense(128, activation = 'relu'))\n",
        "# 2nd Dropout Layer\n",
        "model_1.add(Dropout(0.3))\n",
        "\n",
        "# Final Dense layer => Output prediction untuk multi-class (23 kelas), gunakan softmax untuk mendapatkan probabilitas dari setiap kelas\n",
        "model_1.add(Dense(23, activation='softmax'))\n",
        "######################### Fully Connected Layer ################################\n",
        "\n",
        "######################### Compile Model ################################\n",
        "model_1.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Summary of the Model Architecture\n",
        "print(model_1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hitung jumlah gambar per kelas\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(TRAIN_DIR):\n",
        "    class_path = os.path.join(TRAIN_DIR, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        class_counts[class_name] = len(os.listdir(class_path))\n",
        "\n",
        "# Hitung total semua gambar\n",
        "total_images = sum(class_counts.values())\n",
        "num_classes = len(class_counts)\n",
        "\n",
        "# Hitung bobot per kelas\n",
        "class_weights = {}\n",
        "for idx, (class_name, count) in enumerate(class_counts.items()):\n",
        "    class_weights[idx] = (1 / count) * (total_images / num_classes)\n",
        "=-\n",
        "# Tampilkan hasil\n",
        "for idx, (label, weight) in enumerate(zip(class_counts.keys(), class_weights.values())):\n",
        "    print(f\"Label: {label:20s} | Weight: {weight:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True),\n",
        "    ReduceLROnPlateau(patience=5, factor=0.2, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "# Fitting / training model\n",
        "history_1 = model_1.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XctzCfzbYCBK"
      },
      "source": [
        "## Evaluasi dan Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKk-ScZWYCBK"
      },
      "outputs": [],
      "source": [
        "acc = history_1.history['accuracy']\n",
        "val_acc = history_1.history['val_accuracy']\n",
        "loss = history_1.history['loss']\n",
        "val_loss = history_1.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_generator.reset()\n",
        "\n",
        "preds_1 = model_1.predict(test_generator, verbose=0)\n",
        "preds_1 = preds_1.copy()\n",
        "\n",
        "# Ambil index kelas tertinggi dari softmax output → kelas prediksi\n",
        "y_pred = np.argmax(preds_1, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Label nama kelas\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_fIsUogYFSk"
      },
      "source": [
        "## Konversi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZvGBpYoYFSl"
      },
      "outputs": [],
      "source": [
        "model_1.save(\"saved_model/model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shutil.rmtree(\"C:/Users/isnai/miniconda3/envs/main-ds/Lib/site-packages/tensorflow_decision_forests\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.path.exists(\"C:/Users/isnai/miniconda3/envs/main-ds/Lib/site-packages/tensorflow_decision_forests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15\n",
        "!pip install tensorflow_decision_forests==1.8.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip uninstall tensorflow_decision_forests -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip list decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install tensorflowjs\n",
        "# !pip install tensorflow==2.15.0 tensorflowjs\n",
        "\n",
        "# Convert model.h5 to model\n",
        "!tensorflowjs_converter --input_format=keras model.h5 tfjs_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DbfEwvvm5U4"
      },
      "source": [
        "## Inference (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue5esMSSm8GQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "main-ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
